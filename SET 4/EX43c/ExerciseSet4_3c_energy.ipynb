{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Introduction to Data Analytics - Exercise set 4, exercise 3c</b></h3>\n",
    "\n",
    "<h4><b>Energy production and consumption datasets</b></h4>\n",
    "<h3 style=\"color: red;\">NOTE! This notebook requires that your Python environment has <i>lxml</i>-module installed (used in web scraping).</h3>\n",
    "<h4>To install: <b style=\"color: red;\">pip3 install lxml</b></h4>\n",
    "<h3>After this, restart your Jupyter kernel (Restart -button in top of notebook).</h3>\n",
    "<img src=\"http://srv.plab.fi/~tuomasv/data_analytics_2023_images/exercise_set_4/energy.png\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tip:</b> You can just click \"Run All\" to see the results of this notebook. Some of these exercises have some heavy plots that can take even 1 minute to produce."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Remember:</b> not every dataset has interesting correlations and/or features (too much randomness or variance usually, or not enough data). Also, not each correlation implies there's a causation. Finally, not every plot is useful, that's why it's important to try multiple plots in order to \"see under the hood\". Typically pair plots, joint plots, regression plots, heatmaps and box plots are most useful, but there are some exceptions too."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Quick summary of data:</b></h3>\n",
    "\n",
    "<p>This is a combination of four datasets: coal consumption, nuclear production, wind power production and hydro power production in Texas area.</p>\n",
    "<p>This dataset demonstrates how multiple similar datasets can be combined into a single dataset. Two of the datasets are csv-files, the two others are downloaded from Wikipedia (we will practice web scraping in a later lecture).</p><p>In addition, it's a perfect example why normalization is a good technique, when you have similar datasets, but they have completely different value ranges, thus making comparison very difficult. In a nutshell: normalization converts every value in a column to be between 0.0 and 1.0. Therefore, you can compare columns that have similar trends, but completely different value ranges. For example, comparing gold prices to silver prices.</p>\n",
    "<p>Coal consumption and nuclear data: <a href=\"https://github.com/rishabh89007/Time_Series_Datasets\">https://github.com/rishabh89007/Time_Series_Datasets</a></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b>Questions for this exercise:</b></h3>\n",
    "<b>Answer the questions either as code comments or as markdown. Use separate cells to write your answers.</b><br/><br/>\n",
    "<li>Compare the usefulness of all plots in this notebook. Which approach is the most preferred way, and why?</li>\n",
    "<li>What can you say about the trends of coal, nuclear, hydro and wind power within these four datasets?</li><br />\n",
    "<b>Extra questions/tasks for extra points:</b>\n",
    "<li>Can you see any seasonality in different energy formats? (is some certain energy format used more in certain quarters?)</li>\n",
    "<li>Any other ideas about this dataset? Any ways to expand or improve the dataset and the analysis?</li>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Imports and load the original dataset</b></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the needed modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Helper functions for dataset cleaning</b></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp to quarter (1-4)\n",
    "def timestamp_to_quarter(row):\n",
    "    # get and split the Year-column\n",
    "    value = row['Year']\n",
    "    parts = value.split(\" \")\n",
    "    date = parts[1]\n",
    "    date = date[0:2]\n",
    "    month = int(date)\n",
    "    \n",
    "    quarter = 0\n",
    "    \n",
    "    # determine quarter based on month number\n",
    "    if 1 <= month <= 3:\n",
    "        quarter = 1\n",
    "    elif 4 <= month <= 6:\n",
    "        quarter = 2\n",
    "    elif 7 <= month <= 9:\n",
    "        quarter = 3\n",
    "    elif 10 <= month <= 12:\n",
    "        quarter = 4\n",
    "    \n",
    "    return quarter\n",
    "\n",
    "# month name to quarter (1-4)\n",
    "def month_name_to_quarter(row):\n",
    "    # which month belongs to which quarter\n",
    "    quarter_1 = ['Jan', 'Feb', 'Mar']\n",
    "    quarter_2 = ['Apr', 'May', 'Jun']\n",
    "    quarter_3 = ['Jul', 'Aug', 'Sep']\n",
    "    quarter_4 = ['Oct', 'Nov', 'Dec']\n",
    "    \n",
    "    # determine quarter\n",
    "    if row['Month'] in quarter_1:\n",
    "        return 1\n",
    "    elif row['Month'] in quarter_2:\n",
    "        return 2\n",
    "    elif row['Month'] in quarter_3:\n",
    "        return 3\n",
    "    elif row['Month'] in quarter_4:\n",
    "        return 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Processing data</b></h4>\n",
    "<p>Note: this coding section is very complex. In a nutshell, it loads four different datasets (coal, nuclear, hydro, wind) and combines them into one DataFrame. Also, we create a normalized DataFrame, that allows us the compare the price trends of all energy formats. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATAFRAME 1 #\n",
    "\n",
    "# load coal dataset\n",
    "df1 = pd.read_csv('coalconsumption.csv')\n",
    "\n",
    "# rename the unnamed column to year + quarter\n",
    "df1.rename( columns={'Unnamed: 0':'YearQuarter'}, inplace=True)\n",
    "\n",
    "# rename the column name to more usable: Coal\n",
    "df1.rename(columns={'Total consumption : Texas : electric power (total) : quarterly (short tons)': \"Coal\"}, inplace=True)\n",
    "\n",
    "# split Year and Quarter, and remove original column\n",
    "df1['Year'] = df1['YearQuarter'].str.slice(0, 4).astype(int)\n",
    "df1['Quarter'] = df1['YearQuarter'].str.slice(6, 7).astype(int) \n",
    "df1 = df1.drop('YearQuarter', axis=1)\n",
    "\n",
    "# filter only the three columns we actually need\n",
    "df1 = df1[['Year', 'Coal', 'Quarter']]\n",
    "\n",
    "# only accept data from 2007 onwards in order to keep data integrity within all datasets\n",
    "df1 = df1[df1['Year'].astype(int) >= 2007]\n",
    "\n",
    "# DATAFRAME 2 #\n",
    "\n",
    "# load nuclear dataset, rename the unnamed year column\n",
    "df2 = pd.read_csv(\"nuclear.csv\")\n",
    "df2.rename( columns={'Unnamed: 0':'Year'}, inplace=True)\n",
    "\n",
    "# create quarter number with function, see code above\n",
    "df2['Quarter'] = df2.apply(timestamp_to_quarter, axis=1)\n",
    "\n",
    "# filter only the year from the year column\n",
    "df2['Year'] = df2['Year'].str.slice(0,4).astype(int)\n",
    "\n",
    "# remove values after 2020 in order to keep data integrity within all datasets\n",
    "df2 = df2[df2['Year'].astype(int) <= 2020]\n",
    "\n",
    "# rename the column to something more useful\n",
    "df2.rename(columns={\"U.S. nuclear capacity, daily (Megawatts)\": \"Nuclear\"}, inplace=True)\n",
    "\n",
    "# only allow years from 2007 onwards, in order to keep data integriy within all datasets\n",
    "df2 = df2[df2['Year'].astype(int) >= 2007]\n",
    "\n",
    "# DATAFRAME 3 #\n",
    "\n",
    "# create DataFrame from a Wikipedia page table\n",
    "df3 = pd.read_html('https://en.wikipedia.org/wiki/Wind_power_in_Texas')[3]\n",
    "\n",
    "# simplify complex index\n",
    "df3.columns = df3.columns.to_flat_index()\n",
    "\n",
    "# probably not the most optimal way to clean the columns\n",
    "# but here goes with a traditional loop\n",
    "index = 0\n",
    "\n",
    "for c in df3.columns:\n",
    "    cols = str(c)\n",
    "    parts = cols.split(\", \")\n",
    "    colname = parts[2]\n",
    "    colname = colname.replace(\"'\", \"\")\n",
    "    colname = colname.replace(\")\", \"\")\n",
    "    df3.columns.values[index] = colname\n",
    "    index += 1\n",
    "    \n",
    "# drop the Total column\n",
    "df3 = df3.drop(\"Total\", axis=1)\n",
    "\n",
    "# let's use melt again to get time series data into more usable format (and compatible to previous dataframes)\n",
    "df3_copy = df3.copy()\n",
    "df3 = df3.melt(id_vars=[\"Year\"], \n",
    "              var_name=\"Month\",\n",
    "              value_name=\"Wind\")\n",
    "\n",
    "# set quarter by using our own function\n",
    "df3['Quarter'] = df3.apply(month_name_to_quarter, axis=1)\n",
    "\n",
    "# drop old month column and filter data that is over year 2020 or before 2007\n",
    "df3 = df3.drop('Month', axis=1)\n",
    "df3 = df3[df3['Year'].astype(int) <= 2020]\n",
    "df3 = df3[df3['Year'].astype(int) >= 2007]\n",
    "\n",
    "# DATAFRAME 4 #\n",
    "\n",
    "# load dataframe from Wikipedia page table\n",
    "df4 = pd.read_html('https://en.wikipedia.org/wiki/Hydroelectric_power_in_the_United_States')[3]\n",
    "\n",
    "# remove last row (a summary row) and remove some sum-columns, not needed\n",
    "df4 = df4[:-1]\n",
    "df4 = df4.drop('% of total', axis=1)\n",
    "df4 = df4.drop('Total', axis=1)\n",
    "\n",
    "# filter the correct years again\n",
    "df4 = df4[df4['Year'].astype(int) <= 2020]\n",
    "df4 = df4[df4['Year'].astype(int) >= 2007]\n",
    "\n",
    "# use melt to get data into needed format\n",
    "df4 = df4.melt(id_vars=[\"Year\"], \n",
    "              var_name=\"Month\",\n",
    "              value_name=\"Hydro\")\n",
    "\n",
    "# create quarter, use custom function\n",
    "df4['Quarter'] = df4.apply(month_name_to_quarter, axis=1)\n",
    "\n",
    "# remove old Month-column, set year to integer format (better for heatmaps etc.)\n",
    "df4 = df4.drop('Month', axis=1)\n",
    "df4['Year'] = df4['Year'].astype(int)\n",
    "\n",
    "# COMBINE ALL DATAFRAMES #\n",
    "\n",
    "# group by average quarter values (to remove duplicate quarters) and reset index\n",
    "df2 = df2.groupby(['Year', 'Quarter']).mean().reset_index()\n",
    "df3 = df3.groupby(['Year', 'Quarter']).mean().reset_index()\n",
    "df4 = df4.groupby(['Year', 'Quarter']).mean().reset_index()\n",
    "\n",
    "# merge all the four dataframes, one by one\n",
    "df = pd.merge(df1, df2, on=['Year', 'Quarter'])\n",
    "df = pd.merge(df, df3, on=['Year', 'Quarter'])\n",
    "df = pd.merge(df, df4, on=['Year', 'Quarter'])\n",
    "\n",
    "# create another copy of the merged dataframe \n",
    "normalized_df = df.copy()\n",
    "cols_to_norm = ['Wind', 'Nuclear', 'Coal', 'Hydro']\n",
    "\n",
    "# create normalized values for all energy => values between 0.0 - 1.0. \n",
    "# 0 equals to minimum value in the column, 1 equals to maximum value in the column (relative value)\n",
    "normalized_df[cols_to_norm] = normalized_df[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "# also create melted version\n",
    "df_flipped = df.melt(id_vars=[\"Year\", \"Quarter\"], \n",
    "              var_name=\"Type\",\n",
    "              value_name=\"Total\")\n",
    "\n",
    "# convert total to integer\n",
    "df_flipped['Total'] = df_flipped['Total'].astype(int)\n",
    "\n",
    "# also create melted version of the normalized dataframe\n",
    "normalized_df_flipped = normalized_df.melt(id_vars=[\"Year\", \"Quarter\"], \n",
    "              var_name=\"Type\",\n",
    "              value_name=\"Total\")\n",
    "\n",
    "# create correlation matrices\n",
    "correlations = df.corr(numeric_only=True).round(2)\n",
    "correlations1 = df_flipped.corr(numeric_only=True).round(2)\n",
    "correlations2 = normalized_df.corr(numeric_only=True).round(2)\n",
    "correlations3 = normalized_df_flipped.corr(numeric_only=True).round(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>seaborn plots - pair plots</b></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic pair plot\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>seaborn plots - heatmaps and pivot tables</b></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot table + heatmap for Wind\n",
    "pt = df.pivot_table(index='Quarter', columns='Year', values='Wind')\n",
    "sns.heatmap(pt, cmap='magma') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot table + heatmap for Hydro power\n",
    "pt = df.pivot_table(index='Quarter', columns='Year', values='Hydro')\n",
    "sns.heatmap(pt, cmap='magma') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot table + heatmap for Coal\n",
    "pt = df.pivot_table(index='Quarter', columns='Year', values='Coal')\n",
    "sns.heatmap(pt, cmap='magma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot table + heatmap for Nuclear energy\n",
    "pt = df.pivot_table(index='Quarter', columns='Year', values='Nuclear')\n",
    "sns.heatmap(pt, cmap='magma') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>seaborn plots - pair plots for melted and normalized DataFrames</b></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair plot, hue for type, no normalization\n",
    "sns.pairplot(df_flipped, hue='Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair plot, hue for type, using normalization\n",
    "sns.pairplot(normalized_df_flipped, hue='Type')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>seaborn plots - regression plots</b></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression plot for total and year, hue on type\n",
    "sns.lmplot(x='Year', y='Total', data=normalized_df_flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression plot for total and year, hue on type\n",
    "sns.lmplot(x='Year', y='Total', data=normalized_df_flipped, hue='Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression plot for quarter and total\n",
    "sns.lmplot(x='Quarter', y='Total', data=normalized_df_flipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression plot for quarter and total, hue on type\n",
    "sns.lmplot(x='Quarter', y='Total', data=normalized_df_flipped, hue='Type')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>seaborn plots - heatmaps for correlation matrices</b></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic heatmap for merged dataframe, not melted\n",
    "sns.heatmap(correlations, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic heatmap for merged dataframe, melted\n",
    "sns.heatmap(correlations1, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic heatmap for normalized dataframe, not melted\n",
    "sns.heatmap(correlations2, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic heatmap for normalized dataframe, melted\n",
    "sns.heatmap(correlations3, annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3bc3bf9eecd44de573c9627ba1e399657281629afda28e4e179890a14ac2cfc2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
